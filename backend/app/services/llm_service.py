# backend/app/services/llm_service.py
from langchain_community.chat_models import ChatOpenAI, ChatOllama
from langchain_ollama import ChatOllama, OllamaLLM

from langchain_core.messages import HumanMessage, AIMessage
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PlaywrightURLLoader,
    UnstructuredFileLoader, 
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPDFLoader,
    UnstructuredWordDocumentLoader
)
import os
from typing import List, Dict, Any, Optional
from ..config import settings
online_models = ["deepseek-chat", "deepseek-reasoner"]
class LLMController:
    """
    ç»Ÿä¸€çš„LLMæ§åˆ¶å™¨ï¼Œè´Ÿè´£å¤„ç†æ‰€æœ‰æ¨¡å‹è°ƒç”¨
    """
    
    def __init__(self):
        self.embeddings = OllamaEmbeddings(model=settings.EMBEDDING_MODEL)
        self.persist_dir = settings.VECTOR_DB_PATH
        self._vectorstore = None
    
    def get_vectorstore(self) -> Optional[Chroma]:
        """è·å–å‘é‡æ•°æ®åº“å®ä¾‹"""
        if self._vectorstore is None:
            try:
                if os.path.exists(self.persist_dir):
                    self._vectorstore = Chroma(
                        persist_directory=self.persist_dir, 
                        embedding_function=self.embeddings
                    )
            except Exception as e:
                print(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return self._vectorstore
    
    def get_llm(self, model_name: str, mode: str = "chat", user_id: int = None, db = None):
        """è·å–LLMå®ä¾‹"""
        if model_name in online_models:
            if model_name.startswith("deepseek"):
                from langchain_deepseek import ChatDeepSeek
                
                # å°è¯•ä»æ•°æ®åº“è·å–ç”¨æˆ·çš„API key
                api_key = settings.DEEPSEEK_API_KEY  # é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
                if user_id and db:
                    from ..services.database_service import DatabaseService
                    user_api_key = DatabaseService.get_user_api_key_by_provider(user_id, "deepseek", db)
                    if user_api_key and user_api_key.is_active:
                        api_key = user_api_key.api_key
                        print(f"ä½¿ç”¨ç”¨æˆ· {user_id} çš„DeepSeek API key: {api_key[:10]}...{api_key[-4:]}")
                    else:
                        print(f"ç”¨æˆ· {user_id} æ²¡æœ‰æœ‰æ•ˆçš„DeepSeek API keyï¼Œä½¿ç”¨å…¨å±€é…ç½®")
                else:
                    print(f"ä½¿ç”¨å…¨å±€DeepSeek API key: {api_key[:10]}...{api_key[-4:]}")
                
                # åœ¨çº¿æ¨¡å‹
                return ChatDeepSeek(
                    model=model_name,
                    temperature=settings.DEFAULT_TEMPERATURE,
                    api_key=api_key
                )
            elif model_name.startswith("gpt"):
                # OpenAIæ¨¡å‹
                api_key = settings.OPENAI_API_KEY  # é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
                if user_id and db:
                    from ..services.database_service import DatabaseService
                    user_api_key = DatabaseService.get_user_api_key_by_provider(user_id, "openai", db)
                    if user_api_key and user_api_key.is_active:
                        api_key = user_api_key.api_key
                        print(f"ä½¿ç”¨ç”¨æˆ· {user_id} çš„OpenAI API key: {api_key[:10]}...{api_key[-4:]}")
                    else:
                        print(f"ç”¨æˆ· {user_id} æ²¡æœ‰æœ‰æ•ˆçš„OpenAI API keyï¼Œä½¿ç”¨å…¨å±€é…ç½®")
                else:
                    print(f"ä½¿ç”¨å…¨å±€OpenAI API key: {api_key[:10]}...{api_key[-4:]}")
                
                return ChatOpenAI(
                    model=model_name,
                    temperature=settings.DEFAULT_TEMPERATURE,
                    api_key=api_key
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åœ¨çº¿æ¨¡å‹: {model_name}")
        else:
            # æœ¬åœ°Ollamaæ¨¡å‹
            if mode == "chat":
                return ChatOllama(
                    model=model_name,
                    temperature=settings.DEFAULT_TEMPERATURE
                )
            elif mode == "generate":
                return OllamaLLM(
                    model=model_name,
                    temperature=settings.DEFAULT_TEMPERATURE
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ¨¡å¼: {mode}")

    # def get_rag_context(self, message: str, user_id: int = None, db = None) -> str:
    #     """ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰"""
    #     if not user_id or not db:
    #         print("æ²¡æœ‰ç”¨æˆ·IDæˆ–æ•°æ®åº“")
    #         return ""
    #     from .knowledgebase_service import knowledge_base_service
    #     return knowledge_base_service.get_rag_context(user_id, message, top_k=3, db=db)
    
    def create_rag_chain(
        self,
        user_id: int,
        model_name: str,
        chain_type: str = "stuff",
        use_reranker: bool = True,
        top_k: int = 5,
        score_threshold: float = 0.5,
        use_wiki: bool = False,  # æ·»åŠ WikiçŸ¥è¯†æ”¯æŒ
        db = None
    ):
        """
        åˆ›å»ºRAG Chain
        
        Args:
            user_id: ç”¨æˆ·ID
            model_name: æ¨¡å‹åç§°
            chain_type: Chainç±»å‹
            use_reranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            use_wiki: æ˜¯å¦ä½¿ç”¨WikiçŸ¥è¯†
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            RAG Chainå®ä¾‹æˆ–None
        """
        try:
            # è·å–LLMå®ä¾‹
            llm = self.get_llm(model_name, mode="chat", user_id=user_id, db=db)
            if not llm:
                return None
            
            # åˆ›å»ºRAG Chain
            from .knowledgebase_service import knowledge_base_service
            return knowledge_base_service.create_rag_chain_for_user(
                user_id=user_id,
                llm=llm,
                chain_type=chain_type,
                use_reranker=use_reranker,
                top_k=top_k,
                score_threshold=score_threshold,
                use_wiki=use_wiki,  # ä¼ é€’Wikiå‚æ•°
                db=db
            )
        except Exception as e:
            print(f"åˆ›å»ºRAG Chainå¤±è´¥: {str(e)}")
            return None
    
    def create_simple_chat_chain(
        self,
        model_name: str,
        user_id: int = None,
        db = None
    ):
        """
        åˆ›å»ºç®€å•èŠå¤©Chainï¼ˆä¸ä½¿ç”¨Promptæ¨¡æ¿ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            user_id: ç”¨æˆ·ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            ç®€å•èŠå¤©Chainå®ä¾‹
        """
        try:
            # è·å–LLMå®ä¾‹
            llm = self.get_llm(model_name, mode="chat", user_id=user_id, db=db)
            if not llm:
                return None
            
            # åˆ›å»ºç®€å•èŠå¤©Chain
            from .knowledgebase_service import knowledge_base_service
            return knowledge_base_service.create_simple_chat_chain(llm)
        except Exception as e:
            print(f"åˆ›å»ºç®€å•èŠå¤©Chainå¤±è´¥: {str(e)}")
            return None
    
    def create_chat_chain(
        self,
        model_name: str,
        prompt_name: str = "chat_default",
        user_id: int = None,
        db = None
    ):
        """
        åˆ›å»ºèŠå¤©Chain
        
        Args:
            model_name: æ¨¡å‹åç§°
            prompt_name: Promptæ¨¡æ¿åç§°
            user_id: ç”¨æˆ·ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            èŠå¤©Chainå®ä¾‹
        """
        try:
            # è·å–LLMå®ä¾‹
            llm = self.get_llm(model_name, mode="chat", user_id=user_id, db=db)
            if not llm:
                return None
            
            # åˆ›å»ºèŠå¤©Chain
            from .knowledgebase_service import knowledge_base_service
            return knowledge_base_service.create_chat_chain(llm, prompt_name)
        except Exception as e:
            print(f"åˆ›å»ºèŠå¤©Chainå¤±è´¥: {str(e)}")
            return None
    
    def create_generation_chain(
        self,
        model_name: str,
        prompt_name: str,
        user_id: int = None,
        db = None
    ):
        """
        åˆ›å»ºç”Ÿæˆä»»åŠ¡Chain
        
        Args:
            model_name: æ¨¡å‹åç§°
            prompt_name: Promptæ¨¡æ¿åç§°
            user_id: ç”¨æˆ·ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            ç”ŸæˆChainå®ä¾‹
        """
        try:
            # è·å–LLMå®ä¾‹
            llm = self.get_llm(model_name, mode="generate", user_id=user_id, db=db)
            if not llm:
                return None
            
            # åˆ›å»ºç”ŸæˆChain
            from .knowledgebase_service import knowledge_base_service
            return knowledge_base_service.create_generation_chain(llm, prompt_name)
        except Exception as e:
            print(f"åˆ›å»ºç”ŸæˆChainå¤±è´¥: {str(e)}")
            return None
    
    def get_available_prompts(self):
        """è·å–å¯ç”¨çš„Promptæ¨¡æ¿"""
        from .knowledgebase_service import knowledge_base_service
        return knowledge_base_service.get_available_prompts()
    
    def get_chain_types(self):
        """è·å–å¯ç”¨çš„Chainç±»å‹"""
        from .knowledgebase_service import knowledge_base_service
        return knowledge_base_service.get_chain_types()
    
    def process_message(self, payload: Dict[str, Any], user_id: int = None, db = None):
        """
        æµå¼å¤„ç†æ¶ˆæ¯ - ç»Ÿä¸€ä½¿ç”¨Chainæ–¹å¼
        """
        message = payload.get("message", "")
        model_name = payload.get("model", settings.DEFAULT_MODEL)
        mode = payload.get("mode", "chat")
        use_rag = payload.get("use_rag", False)
        use_wiki = payload.get("use_wiki", False)  # è·å–Wikiå‚æ•°
        chat_history = payload.get("chat_history", [])
        chain_type = payload.get("chain_type", "stuff")
        # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆé€‚çš„é»˜è®¤prompt
        default_prompt = "generate_default" if mode == "generate" else "chat_default"
        prompt_name = payload.get("prompt_name", default_prompt)
        use_reranker = payload.get("use_reranker", True)
        top_k = payload.get("top_k", 5)
        score_threshold = payload.get("score_threshold", 0.5)
        print("=" * 20, "å¤„ç†æ¶ˆæ¯", "=" * 20)
        print(payload)
        print("=" * 20, "å¤„ç†æ¶ˆæ¯", "=" * 20)
        try:
            # å¦‚æœå¯ç”¨äº†WikiçŸ¥è¯†ï¼Œä¼˜å…ˆä½¿ç”¨RAGæ¨¡å¼
            if use_wiki and not use_rag:
                print("ğŸ” æ£€æµ‹åˆ°WikiçŸ¥è¯†è¯·æ±‚ï¼Œè‡ªåŠ¨å¯ç”¨RAGæ¨¡å¼")
                use_rag = True
            
            if use_rag:
                print("=" * 20, "RAG Chainå¤„ç†", "=" * 20)
                # æ ¹æ®modeé€‰æ‹©ä¸åŒçš„RAG Chain
                if mode == "generate":
                    print("ä½¿ç”¨RAGç”ŸæˆChain")
                    rag_chain = self.create_rag_chain(
                        user_id=user_id,
                        model_name=model_name,
                        chain_type=chain_type,
                        use_reranker=use_reranker,
                        top_k=top_k,
                        score_threshold=score_threshold,
                        use_wiki=use_wiki,  # ä¼ é€’Wikiå‚æ•°
                        db=db
                    )
                else:
                    # é»˜è®¤ä½¿ç”¨èŠå¤©æ¨¡å¼
                    print("ä½¿ç”¨RAGèŠå¤©Chain")
                    rag_chain = self.create_rag_chain(
                        user_id=user_id,
                        model_name=model_name,
                        chain_type=chain_type,
                        use_reranker=use_reranker,
                        top_k=top_k,
                        score_threshold=score_threshold,
                        use_wiki=use_wiki,  # ä¼ é€’Wikiå‚æ•°
                        db=db
                    )
                # rag_test = True
                # if rag_test:
                #     from .knowledgebase_service import knowledge_base_service
                #     rag_context = knowledge_base_service.get_rag_context(user_id=user_id,message=message, top_k=top_k, db=db)
                if rag_chain:
                    # print("=" * 20, "RAG Chainå¤„ç†", "=" * 20)
                    # print(rag_chain)
                    # print("=" * 20, "RAG Chainå¤„ç†", "=" * 20)
                    # æµå¼å¤„ç†RAG Chain
                    if hasattr(rag_chain, 'stream'):
                        print("ä½¿ç”¨RAG Chainæµå¼API")
                        try:
                            in_reasoning = False
                            for chunk in rag_chain.stream({"input": message}):
                                # å¤„ç†æ¨ç†å†…å®¹
                                if hasattr(chunk, "additional_kwargs") and chunk.additional_kwargs.get("reasoning_content"):
                                    reasoning_content = chunk.additional_kwargs["reasoning_content"]
                                    if not in_reasoning:
                                        yield "<think>"
                                        yield " \n"
                                        in_reasoning = True
                                    yield reasoning_content
                                else:
                                    if in_reasoning:
                                        yield " \n"
                                        yield "</think>"
                                        yield " \n\n"
                                        in_reasoning = False
                                    # æ­£å¸¸å†…å®¹è¾“å‡º
                                    if hasattr(chunk, 'content') and chunk.content:
                                        yield chunk.content
                                    elif hasattr(chunk, 'result'):
                                        yield chunk.result
                                    elif hasattr(chunk, 'answer'):
                                        yield chunk.answer
                                    elif hasattr(chunk, 'answer_text'):
                                        yield chunk.answer_text
                                    elif isinstance(chunk, dict):
                                        # å¤„ç†æ–°çš„RunnableBindingæ ¼å¼
                                        if 'answer' in chunk:
                                            yield chunk['answer']
                                        elif 'result' in chunk:
                                            yield chunk['result']
                                        elif 'content' in chunk:
                                            yield chunk['content']
                                        elif 'answer_text' in chunk:
                                            yield chunk['answer_text']
                            
                            if in_reasoning:
                                yield "</think>"
                                yield " \n"
                        except Exception as e:
                            yield f"âŒ RAG Chainæµå¼å¤„ç†å¤±è´¥: {str(e)}"
                    else:
                        # éæµå¼RAG Chain
                        print("ä½¿ç”¨RAG Chainéæµå¼API")
                        try:
                            result = rag_chain.invoke({"input": message})
                            answer = result.answer if hasattr(result, 'answer') else result.result if hasattr(result, 'result') else str(result)
                            
                            # æŒ‰å­—ç¬¦æµå¼è¾“å‡º
                            for char in answer:
                                yield char
                        except Exception as e:
                            yield f"âŒ RAG Chainå¤„ç†å¤±è´¥: {str(e)}"
                else:
                    # å¦‚æœRAG Chainåˆ›å»ºå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯å›é€€
                    print("RAG Chainåˆ›å»ºå¤±è´¥")
                    raise Exception("RAG Chainåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥çŸ¥è¯†åº“é…ç½®")
            else:
                # æ ¹æ®modeé€‰æ‹©ä¸åŒçš„Chainå¤„ç†ï¼ˆä¸ä½¿ç”¨RAGï¼‰
                if mode == "generate":
                    print("=" * 20, "ç”ŸæˆChainå¤„ç†", "=" * 20)
                    # ä½¿ç”¨ç”ŸæˆChain
                    generation_chain = self.create_generation_chain(
                        model_name=model_name,
                        prompt_name=prompt_name,
                        user_id=user_id,
                        db=db
                    )
                    print("=" * 20, "ç”ŸæˆChainå¤„ç†", "=" * 20)
                    print(generation_chain)
                    print("=" * 20, "ç”ŸæˆChainå¤„ç†", "=" * 20)
                    
                    if generation_chain:
                        # æµå¼å¤„ç†ç”ŸæˆChain
                        if hasattr(generation_chain, 'stream'):
                            print("ä½¿ç”¨ç”ŸæˆChainæµå¼API")
                            try:
                                in_reasoning = False
                                for chunk in generation_chain.stream({"input": message}):
                                    # å¤„ç†æ¨ç†å†…å®¹
                                    if hasattr(chunk, "additional_kwargs") and chunk.additional_kwargs.get("reasoning_content"):
                                        print("=" * 20, "å¤„ç†æ¨ç†å†…å®¹", "=" * 20)
                                        print(chunk)
                                        print("=" * 20, "å¤„ç†æ¨ç†å†…å®¹", "=" * 20)
                                        reasoning_content = chunk.additional_kwargs["reasoning_content"]
                                        if not in_reasoning:
                                            yield "<think>"
                                            yield " \n"
                                            in_reasoning = True
                                        yield reasoning_content
                                    else:
                                        if in_reasoning:
                                            yield " \n"
                                            yield "</think>"
                                            yield " \n\n"
                                            in_reasoning = False
                                        print("=" * 20, "å¤„ç†æ­£å¸¸å†…å®¹", "=" * 20)
                                        print(chunk)
                                        print("=" * 20, "å¤„ç†æ­£å¸¸å†…å®¹", "=" * 20)
                                        # æ­£å¸¸å†…å®¹è¾“å‡º
                                        if hasattr(chunk, 'content') and chunk.content:
                                            yield chunk.content
                                        elif hasattr(chunk, 'result'):
                                            yield chunk.result
                                        elif hasattr(chunk, 'answer'):
                                            yield chunk.answer
                                        elif hasattr(chunk, 'answer_text'):
                                            yield chunk.answer_text
                                        elif isinstance(chunk, dict):
                                            # å¤„ç†æ–°çš„RunnableSequenceæ ¼å¼
                                            if 'answer' in chunk:
                                                yield chunk['answer']
                                            elif 'result' in chunk:
                                                yield chunk['result']
                                            elif 'content' in chunk:
                                                yield chunk['content']
                                            elif 'answer_text' in chunk:
                                                yield chunk['answer_text']
                                        else:
                                            yield chunk
                                
                                if in_reasoning:
                                    yield "</think>"
                                    yield " \n"
                            except Exception as e:
                                yield f"âŒ ç”ŸæˆChainæµå¼å¤„ç†å¤±è´¥: {str(e)}"
                        else:
                            # éæµå¼ç”ŸæˆChain
                            print("ä½¿ç”¨ç”ŸæˆChainéæµå¼API")
                            try:
                                result = generation_chain.invoke({"input": message})
                                answer = result.answer if hasattr(result, 'answer') else result.result if hasattr(result, 'result') else str(result)
                                
                                # æŒ‰å­—ç¬¦æµå¼è¾“å‡º
                                for char in answer:
                                    yield char
                            except Exception as e:
                                yield f"âŒ ç”ŸæˆChainå¤„ç†å¤±è´¥: {str(e)}"
                    else:
                        # å¦‚æœç”ŸæˆChainåˆ›å»ºå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯å›é€€
                        print("ç”ŸæˆChainåˆ›å»ºå¤±è´¥")
                        raise Exception("ç”ŸæˆChainåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®")
                    return  # ç”Ÿæˆæ¨¡å¼å¤„ç†å®Œæˆï¼Œç›´æ¥è¿”å›
                else:
                    # é»˜è®¤ä½¿ç”¨èŠå¤©æ¨¡å¼
                    print("=" * 20, "èŠå¤©Chainå¤„ç†", "=" * 20)
                    
                    # åˆå§‹åŒ–chat_chainå˜é‡
                    chat_chain = None
                    
                    # æ ¹æ®æ˜¯å¦æœ‰prompt_nameé€‰æ‹©ä¸åŒçš„èŠå¤©Chain
                    if prompt_name and prompt_name != "chat_default":
                        # ä½¿ç”¨å¸¦Promptæ¨¡æ¿çš„èŠå¤©Chain
                        chat_chain = self.create_chat_chain(
                            model_name=model_name,
                            prompt_name=prompt_name,
                            user_id=user_id,
                            db=db
                        )
                    else:
                        # ä½¿ç”¨ç®€å•èŠå¤©Chain
                        chat_chain = self.create_simple_chat_chain(
                            model_name=model_name,
                            user_id=user_id,
                            db=db
                        )
                
                if chat_chain:
                    # æ„å»ºèŠå¤©å†å²
                    history_messages = []
                    for hist in chat_history:
                        if hist.get("role") == "user":
                            history_messages.append(HumanMessage(content=hist.get("content", "")))
                        elif hist.get("role") == "assistant":
                            history_messages.append(AIMessage(content=hist.get("content", "")))
                    
                    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²ï¼‰
                    full_messages = history_messages + [HumanMessage(content=message)]
                    
                    # æµå¼å¤„ç†
                    if hasattr(chat_chain, 'stream'):
                        print("ä½¿ç”¨ç®€å•èŠå¤©Chainæµå¼API")
                        try:
                            in_reasoning = False
                            for chunk in chat_chain.stream(full_messages):
                                if hasattr(chunk, "additional_kwargs") and chunk.additional_kwargs.get("reasoning_content"):
                                    reasoning_content = chunk.additional_kwargs["reasoning_content"]
                                    if not in_reasoning:
                                        yield "<think>"
                                        yield " \n"
                                        in_reasoning = True
                                    yield reasoning_content
                                else:
                                    if in_reasoning:
                                        yield " \n"
                                        yield "</think>"
                                        yield " \n\n"
                                        in_reasoning = False
                                    if hasattr(chunk, 'content') and chunk.content:
                                        yield chunk.content
                            
                            if in_reasoning:
                                yield "</think>"
                                yield " \n"
                        except Exception as e:
                            yield f"âŒ ç®€å•èŠå¤©Chainæµå¼å¤„ç†å¤±è´¥: {str(e)}"
                    else:
                        # éæµå¼èŠå¤©Chain
                        print("ä½¿ç”¨ç®€å•èŠå¤©Chainéæµå¼API")
                        try:
                            response = chat_chain.invoke(full_messages)
                            content = response.content if hasattr(response, 'content') else str(response)
                            
                            # æŒ‰å­—ç¬¦æµå¼è¾“å‡º
                            for char in content:
                                yield char
                        except Exception as e:
                            yield f"âŒ ç®€å•èŠå¤©Chainå¤„ç†å¤±è´¥: {str(e)}"
                else:
                    # å¦‚æœç®€å•èŠå¤©Chainåˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
                    print("ç®€å•èŠå¤©Chainåˆ›å»ºå¤±è´¥")
                    raise Exception("ç®€å•èŠå¤©Chainåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®")
                    
        except Exception as e:
            yield f"âŒ Chainå¤„ç†å¤±è´¥: {str(e)}"
    
    # def process_document(self, file_path: str, file_type: str) -> List:
    #     """å¤„ç†æ–‡æ¡£å¹¶è¿”å›åˆ†å‰²åçš„æ–‡æ¡£å—"""
    #     print(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}, ç±»å‹: {file_type}")
    #     try:
    #         # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
    #         if file_type in ['txt', 'md']:
    #             loader = TextLoader(file_path)
    #         elif file_type == 'pdf':
    #             try:
    #                 # é¦–å…ˆå°è¯•ä½¿ç”¨ PyPDFLoader
    #                 loader = PyPDFLoader(file_path)
    #             except Exception:
    #                 # å¦‚æœå¤±è´¥,ä½¿ç”¨ UnstructuredPDFLoader ä½œä¸ºåå¤‡
    #                 print("PyPDFLoader å¤±è´¥,ä½¿ç”¨ UnstructuredPDFLoader")
    #                 loader = UnstructuredPDFLoader(file_path)
    #         elif file_type in ['docx', 'doc']:
    #             try:
    #                 # é¦–å…ˆå°è¯•ä½¿ç”¨ Docx2txtLoader
    #                 loader = Docx2txtLoader(file_path)
    #             except Exception:
    #                 # å¦‚æœå¤±è´¥,ä½¿ç”¨ UnstructuredWordDocumentLoader ä½œä¸ºåå¤‡
    #                 print("Docx2txtLoader å¤±è´¥,ä½¿ç”¨ UnstructuredWordDocumentLoader")
    #                 loader = UnstructuredWordDocumentLoader(file_path)
    #         else:
    #             # å¯¹äºå…¶ä»–ç±»å‹æ–‡ä»¶,ä½¿ç”¨é€šç”¨çš„ UnstructuredFileLoader
    #             loader = UnstructuredFileLoader(file_path)
            
    #         documents = loader.load()
            
    #         # æ–‡æœ¬åˆ†å‰²
    #         text_splitter = RecursiveCharacterTextSplitter(
    #             chunk_size=settings.CHUNK_SIZE,
    #             chunk_overlap=settings.CHUNK_OVERLAP,
    #             length_function=len,
    #         )
            
    #         return text_splitter.split_documents(documents)
    #     except Exception as e:
    #         print(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    #         return []
    
    # def add_documents_to_vectorstore(self, documents: List) -> bool:
    #     """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
    #     try:
    #         vectorstore = self.get_vectorstore()
    #         if vectorstore and documents:
    #             vectorstore.add_documents(documents)
    #             print(f"æˆåŠŸæ·»åŠ  {len(documents)} æ¡æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    #             for doc in documents:
    #                 print(doc)
    #             print("=" * 20, "å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£æˆåŠŸ", "=" * 20)
                
    #             return True
    #     except Exception as e:
    #         print(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
    #     return False

# å…¨å±€LLMæ§åˆ¶å™¨å®ä¾‹
llm_controller = LLMController()

# backend/app/routers/chat.py
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from .. import schemas
from ..database import SessionLocal
from ..services.llm_service import llm_controller
from ..services.database_service import DatabaseService
import json
import asyncio
import time
router = APIRouter(prefix="/chat", tags=["chat"])

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@router.post("/stream")
def chat_stream_endpoint(req: schemas.ChatRequest, db: Session = Depends(get_db)):
    """
    æµå¼èŠå¤©æ¥å£
    """
    try:
        # éªŒè¯å¯¹è¯æ˜¯å¦å­˜åœ¨ä¸”å±äºè¯¥ç”¨æˆ·
        conversation = DatabaseService.get_conversation_by_id(
            req.conversation_id, 
            req.user_id, 
            db
        )
        if not conversation:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®"
            )
        
        # è·å–èŠå¤©å†å²
        chat_history = DatabaseService.get_chat_history(req.conversation_id, db)
        
        # æ„å»ºLLMæ§åˆ¶å™¨éœ€è¦çš„payload
        payload = {
            "message": req.message,
            "model": req.model,
            "mode": req.mode,
            "use_rag": req.use_rag,
            "chat_history": chat_history
        }
        stream = req.stream if req.stream is not None else True
        # stream = False
        def generate():
            try:
                # è°ƒç”¨LLMæ§åˆ¶å™¨è¿›è¡Œæµå¼å¤„ç†
                full_response = ""
                for chunk in llm_controller.process_message(payload, req.user_id, db):
                    if chunk.startswith('<think>'):
                        chunk = chunk.replace('<think>', '='*20 + ' AIæ€è€ƒä¸­ğŸ¤” ' )
                    if chunk.endswith('</think>'):
                        chunk = chunk.replace('</think>', '='*20 + ' AIæ€è€ƒç»“æŸ')
                    # print(f"ç»è¿‡å¤„ç†åçš„LLMè¿”å›çš„chunk: {chunk}")
                    full_response += chunk
                    if stream:
                        yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                if not stream:
                    yield f"data: {json.dumps({'chunk': full_response})}\n\n"
                # ä¿å­˜èŠå¤©å†å²åˆ°æ•°æ®åº“
                DatabaseService.save_chat_history(
                    conversation_id=req.conversation_id,
                    message=req.message,
                    response=full_response,
                    model=req.model,
                    db=db
                )
                
                yield f"data: {json.dumps({'done': True})}\n\n"
                
            except Exception as e:
                error_msg = f"âŒ æµå¼å¤„ç†å¤±è´¥: {str(e)}"
                yield f"data: {json.dumps({'error': error_msg})}\n\n"
        
        return StreamingResponse(generate(), media_type="text/plain")
        
    except Exception as e:
        print(f"æµå¼èŠå¤©æ¥å£é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æµå¼èŠå¤©å¤„ç†å¤±è´¥: {str(e)}")
